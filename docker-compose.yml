version: "3"

services:

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENERS: INTERNAL://:9092,OUTSIDE://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,OUTSIDE://host.docker.internal:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
    extra_hosts:
      - "host.docker.internal:192.168.15.1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.11.2
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es01
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./es01:/var/lib/elasticsearch/data
    ports:
      - 9200:9200
    extra_hosts:
      - "host.docker.internal:192.168.15.1"

  kib01:
    image: docker.elastic.co/kibana/kibana:7.11.2
    container_name: kib01
    ports:
      - 5601:5601
    environment:
      ELASTICSEARCH_URL: http://es01:9200
      ELASTICSEARCH_HOSTS: '["http://es01:9200"]'
    extra_hosts:
      - "host.docker.internal:192.168.15.1"
  
  producer:
    build:
      context: ./spark/
      dockerfile: producer.Dockerfile
    depends_on:
      - kafka

  consumer_lula:
    build:
      context: ./spark/
      dockerfile: consumer_lula.Dockerfile
    depends_on:
      - kafka
      - producer
  
  consumer_bolsonaro:
    build:
      context: ./spark/
      dockerfile: consumer_bolsonaro.Dockerfile
    depends_on:
      - kafka
      - producer
  
  consumer_lula_bolsonaro:
    build:
      context: ./spark/
      dockerfile: consumer_lula_bolsonaro.Dockerfile
    depends_on:
      - kafka
      - producer